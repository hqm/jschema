
sbt
run

There are two world Planes - front and back

+ world needs background spatial markers for navigation


What is the API to the sensorimotor system? 

Models:
  blocks with friction, density, texture

   server can implementat 'sticky' blocks



Motor output:

System has two arms. Each has two joints and a hand:

  Gross motor action: upper arm, moves to a quadrant of screen, say 1/10 quantized

  Fine motor action: lower arm, moves 1/10 of quadrant

  Rotator Joints at the shoulder and elbow, i.e., there is redundancy for reaching a point but that
  allows for better reuse of relative motion schemas

  Gross and fine joints have control over force left,right, up/down

  "Lift" pulls a block out of the plane so it can be moved over other blocks.

  - The motor system provides both force and a targeted 'move to proprioceptive position' function. E.g., 
  we've got a fairly high level motor system.

  hand: grasp, ungrasp, rotate left, rotate right
    grasp and rotate force is controllable over some range
     -- the 'grasp' operation gives a force feedback of "grip" when there is an object touching, with force sensation
        proportional to the gripping force, like when you squeeze a block with you fingers.


    HEAD: rotatation , translation

    Gaze: 10x10 gross angle quantization
    	  10x10 fine angle position

Also see Vision system, there is 'motor control' of internal pipeline vision filters, you can
choose which ones go into pipeline

Vocal: phoneme output

Sensor inputs:

  Proprioception: 
     10x10 location for gross motor system - divided into x,y axis? 
     10x10 location for fine motor system 
     shoulder and elbow joint position

Force/touch

     force on shoulder joint, x, y, rotation

     force on elbox x,y,rotation

     force on wrist join rotation, x, y 

     force on grasp

     touch on fingers, texture, temperature, heat conductivity, on fingers. 

     

encoding of magnitudes; should we encode as V>n ? 

v= 6

n = >1 >2 >3 >4 >5 >6 >7 >8 >9 >10

    1  1  1  1  1  1  0  0  0  0




Vision

  modules:

  broad-area vision - blob detection per quadrant, a couple of bits of color/grayscale

  fovea operations:

     closed area fill - is area at fovea closed? 
     are there any bits on at the fovea ; color, density, texture (image morphology bank of filters)


     edge detection -- vertical edges, horizontal edges, diagonal edges (in 30 degree increments

  filter system (image morphology)
    prescale
    erode/dilate
        

   motion tracking; attention focus points on fovea and broad-area peripheral vision  when motion is sensed
     you can enable tracking as an action on the fovea; you then get the gaze moved around for you as it tracks an object
     that is visible.


     Motor action: focus eye on object center of mass. Send hand to gaze position? 

     +      Motion map: at each timestep, compute which objects are moving, light up sensors in a motion quadrant map.

     +      Motion at fovea - has more directional info, velocity. 

     When the head moves, mark all quadrants which have objects as in motion? Or is that something that
     we should handle in the vision system, and factor out by the time it reaches schema mechanism? What can you
     learn from moving head and seeing an object motion vector opposite direction? I see the value of understanding
     its final location, i.e., it's static position visual sensor, but what value is the momentary motion illusion?

     

   trajectory - when motion detected, a motion vector input is stimulated


   Foveation: vision system will pull focus of attention to nearest object, and give a proprioceptive
   sensation of moving the gaze by dx,dy.  Vision hardware can be asked to cycle to nearest objects, in one
   of four directions; left,right,top,bottom. You get sensation of how far the gaze moved to get to next object.
   That way you can trace out the location of several nearby objects, and get a sensation of how they are 
   relatively positioned.



Audio

  Basic frequency burst (pop, click, boom), localization from auditory system to a quadrant
  Phoneme inputs



================================================================

How is sensorimotor info passed to schema engine? 

class SensoryInput {
      int id; // a unique id for this input
      boolean value;
      boolean changed;
}

class MotorAction {
      int id; // unique id for this output
      boolean value;
}

class WorldState {

    HashMap<String,SensorInput> inputList = new HashMap<String,SensorInput>();
    
    HashMap<String,MotorAction> outputList = new HashMap<String,MotorAction>();


}







================
Questions:

How do infants learn to keep still? I.e., it's important to only be moving a few things at time, initially.
So you need to keep most of the body still. And for fine motor activity need to keep the gross motor joints still.

How much of that is built in? You need to set the opposing muscles at
exactly the same force to maintain constant proprioceptive input. In
terms of schema, what are the interesting schemas being learned there,
to maintain that invariant input? Why is it interesting to the Schema mechanism to learn
to keep limbs still by default?  Or to keep head aligned with an object to keep it centered in fovea? 


call David McDonald about language


================================================================
export CLASSPATH=lib/controlP5.jar:lib/core.jar:lib/gluegen-rt.jar:lib/jbox2d_2.1.2_ds_v2.jar:lib/jogl-all.jar:lib/jruby-complete-1.7.4.jar:lib/pbox2d.jar:lib/trove4j-3.0.3.jar:lib/typesafe-config.jar:target/scala-2.10/jschema_2.10-1.1.jar:lib/log4j.jar

jirb 




Web API

http://127.0.0.1:8080/
http://127.0.0.1:8080/items/map
http://127.0.0.1:8080/raw/map




================================================================
Main loop - select an action, perform it once, wait n cycles to
attribute effects to it.

Need to remember last action activated, but only activate it once.

+ Need to bias the gaze to stay centered where the head is.

+ add grasp reflex: = when hand touched, grasp it
 -- preinitialize schema for this ?

+ Do some whole areas of the brain go idle when doing learning, so you might be biased towards just learning
associations between motion and touch, or touch and vision? So that not too many are active at once
to swamp the learning mechanism? 

+ Embellishment - when spinning off new (result) schema B from parent A, add B's synthetic item to the item ignore list
of A. Because any child schema's synthetic item will always be correlated to parent activation.

>>>>Check if Drescher already covered this case with suppress/override heuristic ...

    Hi Henry. Yes, I think that should be suppressed. Also, no synthetic
    item should be defined for a schema with an empty result (a synthetic
    item reifies the validity conditions of a schema, and a schema with an
    empty result has no validity conditions).

    For a schema with a nonempty result, it shouldn't be possible that the
    schema's own synthetic item is discovered to be a result of that
    schema's activation. At most, that schema's activation should
    transition the schema's synthetic item from Unknown to On, not from
    Off to On; and the former transition does not count for the
    extended-result correlation statistics. (If activating a schema were
    to turn its own synthetic item On--which is to say, if activating the
    schema were to achieve the schema's validity conditions--then the
    schema would always be valid, in which case its own synthetic item
    should always be On, rather than being designated as a result of the
    schema.)

+ Check what are the exact conditions under which a schema's synthetic item goes to what state? 

+ "My implementation used an ad hoc method that was tied to its
space-limited statistics collection method. But the real way to do it
is to use a threshold of statistical significance. So just pre-compute
a lookup table that says what the minimum correlation is that can be
supported by a given sample size.
"

+ I'm getting positive feedback in selection/creation; I pick a schema at random to execute,
but if they are based on HAND_UP, then they spin off some HAND_UP schemas, so the random selection
is biased towards moving hand up.

+ How does a touch-grasp  reflex work? Is there a schema for it? it operates outside of schema mechanism,
as an instructor? 

+ How do I implement this (4.1.2) "These statistics are tabulated over
a number of trials in which the action is tak- en, and a number of
trials in which it is not; the more trials there have been, and the
more discrepancy there is between the two probabilities, the sooner
the machinery will detect the difference (see section 5.2.2). The
sampling is weighted toward the most recent trials."


+ When a parent spins off a child result schema for an item, do you reset that item's counters to zero in the parent schema 
extended result? 

sudo apt-get install xvfb
Xvfb :2 -screen 0 1024x768x24 &
export DISPLAY=localhost:2.0
java -jar jschema-assembly-1.1.jar


